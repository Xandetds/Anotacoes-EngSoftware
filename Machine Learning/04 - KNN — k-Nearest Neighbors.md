# KNN ‚Äî k-Nearest Neighbors

## O que √© o KNN

<details>
<summary><strong>O que √© o KNN</strong></summary>

O **k-Nearest Neighbors (k-Vizinhos Mais Pr√≥ximos)** √© um dos algoritmos mais simples de Machine Learning.

Ele √© usado tanto para **classifica√ß√£o** quanto para **regress√£o**, e funciona com base na **proximidade** entre os dados.

Ideia central:

> Quando queremos prever algo novo, o algoritmo procura os ‚Äúk‚Äù exemplos mais parecidos no conjunto de treino e usa as respostas deles para decidir o resultado.
> 

Por isso o nome ‚Äúk-vizinhos mais pr√≥ximos‚Äù.
</details>

---

<details>
<summary><strong>Como o KNN funciona</strong></summary>

## Como o KNN funciona

1. **Guardar os dados**
    
    Diferente de outros modelos, o KNN n√£o aprende uma fun√ß√£o matem√°tica.
    
    Ele apenas **armazena** todos os exemplos do conjunto de treino.
    
2. **Calcular as dist√¢ncias**
    
    Quando chega um novo dado para classificar ou prever, o KNN mede **a dist√¢ncia entre esse novo ponto e todos os pontos j√° conhecidos**.
    
    As dist√¢ncias podem ser calculadas de v√°rias formas (a mais comum √© a dist√¢ncia Euclidiana).
    
    F√≥rmula da dist√¢ncia Euclidiana entre dois pontos A e B:
    
    $$
    
    d(A,B) = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
    
    $$
    
3. **Selecionar os k vizinhos mais pr√≥ximos**
    
    Depois de calcular todas as dist√¢ncias, o algoritmo pega os **k menores valores,** ou seja, os vizinhos mais pr√≥ximos do ponto que queremos prever.
    
4. **Decidir a resposta**
    - **Para classifica√ß√£o:** o KNN escolhe a **classe mais comum** entre os vizinhos.
    (Exemplo: se dos 5 vizinhos, 3 s√£o da classe A e 2 s√£o da classe B, o resultado √© A.)
    - **Para regress√£o:** o KNN tira a **m√©dia** dos valores dos vizinhos.
</details>

---

<details>
<summary><strong>Exemplo pr√°tico</strong></summary>

## Exemplo pr√°tico

Um modelo que tenta prever o tipo de fruta com base em duas caracter√≠sticas: peso e textura.

Quando inserimos uma nova fruta, o KNN vai:

1. Calcular a dist√¢ncia dela para todas as frutas conhecidas.
2. Pegar as 3 mais pr√≥ximas (por exemplo, `k = 3`).
3. Ver quais classes aparecem com mais frequ√™ncia entre essas 3 frutas.
4. Classificar a nova fruta como a classe predominante.
</details>

---

<details>
<summary><strong>Escolha do valor de k</strong></summary>

## Escolha do valor de k

O valor de k √© um dos fatores mais importantes para o desempenho do KNN.

- Um k pequeno (como 1 ou 2) pode deixar o modelo muito sens√≠vel a ru√≠dos e outliers.
- Um k muito grande pode deixar o modelo gen√©rico demais, ignorando detalhes importantes.

Em geral, o valor de k √© escolhido com base em experimentos com o conjunto de valida√ß√£o.
</details>

---

<details>
<summary><strong>Tipos de dist√¢ncia</strong></summary>

## Tipos de dist√¢ncia

O KNN pode usar diferentes maneiras de calcular o que significa ‚Äúestar pr√≥ximo‚Äù.

As principais s√£o:

| Tipo de Dist√¢ncia | Descri√ß√£o | Exemplo |
| --- | --- | --- |
| **Euclidiana** | Dist√¢ncia em linha reta entre dois pontos. | Caminho direto. |
| **Manhattan** | Dist√¢ncia somando os deslocamentos horizontais e verticais. | Como andar em ruas de um quarteir√£o. |
| **Chebyshev** | Considera apenas o maior deslocamento em qualquer dimens√£o. | Movimento de uma torre no xadrez. |

A escolha da m√©trica depende do tipo de dado e da rela√ß√£o entre as vari√°veis.
</details>

---

<details>
<summary><strong>Vantagens e desvantagens</strong></summary>

## Vantagens e desvantagens

### Vantagens

- Simples de entender e implementar.
- N√£o precisa de treinamento.
- Funciona bem com dados pequenos e bem distribu√≠dos.

### Desvantagens

- Fica lento com muitos dados, pois precisa comparar o novo ponto com todos os outros.
- Depende fortemente da escala das vari√°veis (√© importante normalizar os dados).
- Pode se confundir com dados muito ruidosos ou com classes desbalanceadas.
</details>

---

<details>
<summary><strong>Quando usar o KNN</strong></summary>

## Quando usar o KNN

O KNN √© bom para:

- Problemas simples de classifica√ß√£o, onde os dados t√™m poucas dimens√µes.
- Casos onde os padr√µes s√£o bem definidos (como imagens pequenas, dados m√©dicos, ou datasets de treino organizados).

Mas n√£o √© quando:

- O conjunto de dados √© muito grande (fica lento).
- Existem muitas vari√°veis.
- As classes t√™m tamanhos muito diferentes.
</details>

---

<details>
<summary><strong>Exemplo de c√≥digo pr√°tico</strong></summary>

## Exemplo de c√≥digo pr√°tico

O algoritmo KNN foi implementado e testado em meus experimentos no reposit√≥rio de Machine Learning,

na se√ß√£o de **Foundations ‚Üí Notebooks**, que cobre exemplos do livro *Introduction to Machine Learning with Python*.

üîó Veja a implementa√ß√£o completa:
[Reposit√≥rio Machine Learning Labs ‚Äî Foundations (KNN)](https://github.com/Xandetds/machine-learning-labs/tree/main/0_FOUNDATIONS/notebooks)

Abaixo, um exemplo resumido de como o algoritmo √© implementado com a biblioteca `scikit-learn`:

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Carregar o dataset
X, y = load_iris(return_X_y=True)

# Dividir em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Criar o modelo com k=3
model = KNeighborsClassifier(n_neighbors=3)

# Treinar e prever
model.fit(X_train, y_train)
pred = model.predict(X_test)

# Avaliar
print("Acur√°cia:", accuracy_score(y_test, pred))
